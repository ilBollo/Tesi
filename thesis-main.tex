\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{array}   
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

\lstdefinelanguage{json}{
    basicstyle=\footnotesize\ttfamily,numbers=none,breaklines=true,
    frame=lines,backgroundcolor=\color{gray!10},showstringspaces=false,
    string=[db]{"},stringstyle=\color{green!50!black},
    morestring=[s][\color{black}]{\ \ "}{":},keywordstyle=\color{blue},
    keywords={true,false,null},literate=*{0}{{{\color{red}0}}}{1}
    {1}{{{\color{red}1}}}{1}{2}{{{\color{red}2}}}{1}{3}{{{\color{red}3}}}{1}
    {4}{{{\color{red}4}}}{1}{5}{{{\color{red}5}}}{1}{6}{{{\color{red}6}}}{1}
    {7}{{{\color{red}7}}}{1}{8}{{{\color{red}8}}}{1}{9}{{{\color{red}9}}}{1}
    {.}{{{\color{red}.}}}{1}{:}{{{\color{gray}{:}}}}{1}{,}{{{\color{gray}{,}}}}{1}
    {\{}{{{\color{gray}{\{}}}}{1}{\}}{{{\color{gray}{\}}}}}{1}{[}{{{\color{gray}{[}}}}{1}{]}{{{\color{gray}{]}}}}{1}
}

\usetikzlibrary{shapes,arrows,positioning}
\usetikzlibrary{fit}
\usetikzlibrary{calc}
\tikzstyle{block} = [rectangle, draw, fill=white, 
    text width=3cm, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']

\school{\unibo}
\programme{Corso di Laurea in Ingegneria e Scienze Informatiche}
\title{Integrazione di RAG e LLM nello Sviluppo del Software}
\author{Bollini Simone}
\date{\today}
\subject{Programmazione ad oggetti}
\supervisor{Prof. Viroli Mirko}
\cosupervisor{Dott. Aguzzi Gianluca}
%\morecosupervisor{Dott. Farabegoli Nicolas}
\session{IV}
\academicyear{2023-2024}

% Definition of acronyms
\acrodef{RAG}{Retrieval-Augmented Generation}
\acrodef{AI}{Artificial intelligence}
\acrodef{LLM}{Large Language Model}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}

\frontmatter \frontispiece

\begin{abstract}	
I \ac{LLM} addestrati per sviluppare il codice sono oggi altamente efficaci e in grado di generare soluzioni di qualità.
L'addestramento fatto sui modelli è però su fonti generali, questo non da quindi la possibilità al modello di generare soluzioni su misura per una specifica richiesta utilizzando codice già creato dal programmatore o dalla propria azienda per casi simili.
Da questo nasce l'esigenza di addestrare il modello per personalizzare le soluzioni proposte, contestualizzandole alla propria realtà aziendale e al proprio stile nel programmare ma
il fine-tuning di un \ac{LLM} è un processo molto costoso e difficile da utilizzare per mantenere aggiornato frequentemente la base di conoscenza del modello.
Per rispondere a questa esigenza entra in gioco la \ac{RAG}, che permette di aumentare la conoscenza del modello, recuperando informazioni da una propria base di conoscenza, esterna al modello, come librerie specifiche di un azienda, arricchendo il prompt della query di input che sarà elaborata dal LLM.
Il \ac{RAG}, ricercando semanticamente i chunk maggiormente somiglianti a quanto richiesto se trovati, li inserirà per aumentare il prompt del LLM, estendendo la base di informazioni sulla quale genererà l'output con la risposta.
Questa tesi approfondisce questi concetti e sperimenta l'integrazione di un \ac{RAG} specifico per codice Java e un \ac{LLM} con lo scopo di ottenere risposte personalizzate
che solo con la conoscenza del LLM, anche se estremamente performante e completo, sarebbe stato impossibile ottenere.
\end{abstract}

\begin{dedication}
A Giulia e ai miei figli, il dono più incredibile di questa vita.
\newline Alla mia grande famiglia.
\newline Grazie
\end{dedication}

%----------------------------------------------------------------------------------------
\tableofcontents   
%\listoffigures     % (optional) comment if empty
%\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduzione}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

\section{Essere programmatori nel 2025}
Per sviluppare codice sono disponibili tantissimi (IDE), uno di questi è \textbf{Visual Studio Code},
mentre per condividere i progetti e lavorare in team lo strumento utilizzato potrebbe essere  \textbf{GitHub}.
Se richiesta memoria GPU per piccoli progetti accademici è posssibile utilizzare \textbf{COLAB}, che permette di eseguire in remoto codice Python utilizzando GPU senza costi.
Questi esempi sono parte di una panoramica di strumenti sempre più vasta, complessa e in rapita evoluzione, con un frequente cambio di software per realizzare un programma.
Inoltre i possibili modi per realizzare i progetti è aumentanta, disponendo oggi di sempre più librerie e metodi per ottenere risultati finali simili.
Un esempio d'utilizzo degli strumenti sopra elencati potrebbe seguire la seguente scaletta:
\begin{itemize}
    \item Realizzazione iniziale del progetto in locale utilizzado Visual Studio Code
    \item Push del codice su GitHub per condividere il progetto con il team
    \item Esecuzione del codice su Colab per testare il codice su GPU ed eventualmente eseguire modifiche concluse con nuovo Push su GitHub
    \item Pull in locale per continuare a lavorare sul progetto
\end{itemize}
Questo modo dinamico di lavorare è recente ma non una novità, come invece lo sono alcuni specifici strumenti forniti da questi software.
Questi strumenti sono tutti accumunati \textbf{dall'implementazione al loro interno di funzioni basate sull'IA}.
Queste funzioni sono in grado di completare il codice, suggerire correzioni e creare documentazione pertinente.
\textbf{GitHub} ha introdotto \textbf{Copilot}, un assistente IA per la scrittura del codice, questo strumento è integrabile con vari IDE tra cui proprio \textbf{Visual Studo Code}.
Un esempio semplice ma che offre già un idea della vastità e della potenza di queste funzioni è l'utility di \textbf{Github Copilot}: \emph{'Generate Commit Message with Copilot'}
che propone il testo da utilizzare come descrizione di un commit,
ho provato a verificare quanto fosse contestualizzato e coerente 
con le modifiche apportate al codice ed ho ottenuto il seguente risultato:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/commit.png}
    \label{fig:Commit-Autogenerato}
\end{figure}
\newline
Nel mio caso quanto proposto era corretto ed ho quindi eseguito il Commit con la descrizione proposta.
Quanto è riuscito a fare Copilot è strabiliante, in pochi istanti ha analizzato il contesto ritornando come output una risposta semplice ma coerente rispetto a quanto cambiato.
L'uso di questi strumenti sta rendendo il lavoro molto più dinamico e veloce, riducendo le interruzioni nel cercare soluzioni o per trovare le giuste parole per descrivere
quanto fatto.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/copilotsolutionSettimanaEnigmistica.png}
    \label{fig:Copilot-Solution}
\end{figure}
\newline
L'intelligenza artificiale sta rivoluzionando il modo in cui il software viene sviluppato, strumenti come Copilot utilizzando tutto il loro potenziale, possono creare la spina dorsale di un progetto
in poco tempo lasciando al programmatore il compito di verificare e correggere solo in parte il codice proposto. 
In progetti complessi questo non riduce il ruolo del programmatore, anzi lo eleva a compiti di precisione e ad alto valore aggiunto delegando la stesura di parti del codice semplici e ripetitive al software stesso.
Per questi motivi capire come funzionano oggi questi strumenti è importante, sapere come chiedere e formulare correttamente le domande al LLM è fondamentale, esplicitando nel dettaglio con parole chiave mirate come deve essere realizzato il codice per indirizzarlo nell'elaborazione e ragionamento corretto.
Altro compito complesso per il programmatore è non farsi troppo ammaliare dalle soluzioni proposte perché non sempre necessarie per quanto richiesto oppure diverse da quanto già conosciuto
 per realizzare una determinata funzione.
Questo nuovo modo di lavorare permette di conoscere nuove soluzioni ma comporta test e tempo non sempre disponibile,
il programmatore deve sempre avere il controllo del progetto, accettando generazione del codice automatica solo dove consapevole di quanto proposto e del suo impatto anche in casi di revisione e manutenzione futuri.
Il codice deve rimanere rapidamente leggibile e coerente in tutte le sue parti, far generare il codice in automatico può portare ad una perdità di coerenza e leggibilità. 
Proprio per questo l'ultimo miglio da percorrere per sfruttare questi strumenti è la personalizzazione delle risposte del LLM, per ottenere risposte rimanendo nel contesto e nello stile di quanto già realizzato e conosciuto, per fare questo entra in gioco il \textbf{Fine-Tuning} e i \textbf{RAG} che verrano ampiamente approfonditi.

\chapter{Addestrare un LLM per la Generazione del Codice}

L'addestramento di LLM per la generazione del codice di programmazione richiede una serie di passaggi complessi e costi significativi.
Conoscere questo processo è utile per poter poi comprendere al meglio la successiva implementazione con le tecniche di \textbf{RAG}.
La procedura si divide nelle seguenti fasi:
\section{Scelta Modello}
Gli LLM utilizzano tipicamente architetture basate su trasformatori, che sono particolarmente efficaci nell'elaborazione di sequenze di dati, come il testo e il codice.
I trasformatori utilizzano meccanismi di auto-attenzione per valutare l'importanza di diversi elementi in una sequenza,
permettendo al modello di comprendere le relazioni tra parole o token.
Questa capacità è fondamentale nella generazione del codice, poiché le dipendenze tra variabili e funzioni possono estendersi su ampie sezioni del codice, richiedendo al modello di considerare un ampio contesto per trovare le risposte corrette.
L'architettura del modello scelto influenzerà in maniera decisiva tutte le successive fasi di addestramento.
È utile notare che sebbene i trasformatori siano attualmente lo standard, esistono anche altri approcci come le reti neurali ricorrenti (RNN e LSTM) e nuove tecniche in continua evoluzione come i Large Concept Models \cite{code-llm-survey-2024}.
\section{Raccolta e Preparazione dei Dataset}
La qualità e la quantità dei dati per l'addestramento è di primaria importanza per prepare un modello alla generazione di codice in maniera efficace.
È quindi essenziale utilizzare per il training codice proveniente da molteplici fonti tra cui codice sorgente, file readme, documentazione tecnica, commenti nel codice,
pagine Wiki, API e discussioni su forum specializzati in programmazione, arricchendo così il dataset con esempi pratici e ricchezza terminologica.
In rete è possibile trovare diverso materiale open source tra cui dataset già etichettati.
Alcuni dataset hanno un valore altissimo, per questo motivo per tutelare il costo speso per produrli per certi dataset è previsto il diritto d'autore.
I dati si dividono in due tipologie:
\begin{itemize}
    \item \textbf{Dati Strutturati}: seguono un formato specifico e predefinito, seguono la struttora in coppie (descrizione, codice).
    \item \textbf{Dati non Strutturati}: non sono organizzati e sono quindi più difficili da interpretare dal modello. 
\end{itemize}
\subsection{Pulizia e Pre-Processo}
La raccolta di dati va visionata con cura, se non si conosce la provenienza del codice è possibile che contenga bug o codice obsoleto che possono essere trasmessi al modello.
Con la rapida evoluzione del codice molte librerie e tecniche vengono \textbf{rapidamente deprecate} e superate per questo anche utilizzando i più noti modelli LLM ad oggi disponibili, può capitare di ricevere come output \textbf{codice obsoleto che risolve il quesito ma con soluzioni contenti tecniche, api e librerie deprecate o non più disponibili.}
Per questo motivo i dati raccolti devono essere puliti e pre-processati per rimuovere errori e informazioni non pertinenti, garantendo così un dataset di alta qualità per l'addestramento.
\subsubsection{Tokenizzazione}
Il modello per poter elaborare il dataset ha bisogno che quest'ultimo venga diviso in parti più piccole chiamate token per mantenere l'integrita del dato \cite{stanford-codegen},
i token possono essere parole, parti di parole o singoli caratteri, e questa suddivisione è fondamentale per:

\begin{itemize}
    \item \textbf{Gestione del contesto}: mantenere la relazione semantica tra i diversi elementi del codice
    \item \textbf{Efficienza computazionale}: processare grandi quantità di testo in modo ottimizzato
    \item \textbf{Limitazioni del modello}: rispettare i limiti massimi di input del modello (tipicamente tra 512 e 4096 token)
    \item \textbf{Preservazione della struttura}: mantenere la struttura sintattica del codice sorgente
\end{itemize}

Ad esempio, nel codice Java, i token potrebbero includere:
\begin{itemize}
    \item Parole chiave (\texttt{public}, \texttt{class}, \texttt{static})
    \item Identificatori (nomi di variabili e metodi)
    \item Operatori e simboli (\texttt{+}, \texttt{=}, \texttt{\{}, \texttt{\}})
    \item Stringhe letterali e commenti
\end{itemize}
\section{Pre-Addestramento}
Il pre-addestramento di un LLM specializzato nella generazione di codice ha lo scopo di fornire al modello una conoscenza generale della sintassi e delle strutture logiche dei linguaggio di programmazione.
Durante questa fase il modello impare a generare codice partendo da dati non etichettati utilizzando tecniche come il \emph{language modeling} autoregressivo per insegnare al modello di predire il token successivo in una sequenza.
Questo approccio rende la generazione contestualmente e coerente di codice, sfruttando la capacità del modello di "ricordare" il contesto anche su ampie sequenze di dati.
\section{Fine-Tuning}
Il fine-tuning è la fase in cui il modello già pre-addestrato viene ulteriormente specializzato per la generazione di codice adattando e migliorando il modello per specifici domini di applicazione.
Durante questa fase, il modello affina le sue capacità attraverso dataset specializzati composti da coppie descrizione-codice, documentazione tecnica e commenti, esempi di bug-fixing e refactoring.
\subsection{Tecniche di Apprendimento}:
    \begin{itemize}
        \item \textbf{Supervisionato}: Training su coppie input-output predefinite, il modello impara a mappare input di descrizione con linguaggio naturale a output di codice corrispondente.
        \item \textbf{Per Rinforzo}: Ottimizzazione basata su feedback e metriche di qualità
        \item \textbf{Few-shot Learning}: Adattamento a nuovi contesti con pochi esempi
    \end{itemize}

\subsection{Overfitting}
Il processo di fine-tuning richiede un attento bilanciamento nell'apprendere dai dati di addestramento cercando di evitare di incorrere in overfitting.
L'overfitting si verifica quando il modello si specializza troppo sui dati di addestramento, riducendo la sua capacità di generalizzazione producendo risposte errate o incoerenti su nuovi dati.
Per evitare l'overfitting vengono utilizzati set di validazione, regolarizzazione e tecniche di dropout.
\section{Pre-Addestramento vs Fine-Tuning}
È importante comprendere la distinzione tra queste due fasi:
\subsubsection{Pre-Addestramento}
Il pre-addestramento è la fase iniziale dove il modello:
\begin{itemize}
    \item Acquisisce una comprensione \textbf{generale} del linguaggio di programmazione
    \item Viene addestrato su \textbf{grandi quantità} di codice sorgente generico
    \item Impara le strutture base e la sintassi del linguaggio
    \item Non è ancora specializzato per compiti specifici
\end{itemize}

\subsubsection{Fine-Tuning}
Il fine-tuning è invece la fase di specializzazione dove il modello:
\begin{itemize}
    \item Si adatta a un \textbf{dominio specifico} o a compiti particolari
    \item Utilizza dataset specifici e composti da dati strutturati 
\end{itemize}
\section{Valutazione e Ottimizzazione}
Una volta addestrato, il modello deve essere rigorosamente valutato utilizzando metriche specifiche per la generazione di codice, come la correttezza sintattica, la funzionalità e l'efficienza del codice prodotto.
I risultati della valutazione possono essere utilizzati per ulteriori ottimizzazioni, come aggiustamenti dei pesi del modello, modifiche all'architettura o includere dati di addestramento aggiuntivi per affrontare eventuali carenze.

\subsection{Metriche di Valutazione}
\begin{itemize}
    \item \textbf{Correttezza Sintattica}: Verifica che il codice generato sia sintatticamente corretto.
    \item \textbf{Funzionalità}: Verifica che il codice generato realizzi la funzionalità desiderata.
    \item \textbf{Efficienza}: Valuta le prestazioni del codice in termini di tempo di esecuzione e utilizzo delle risorse.
\end{itemize}

\subsection{Tecniche di Ottimizzazione}
\begin{itemize}
    \item \textbf{Aggiustamento dei Pesi}: Modifica dei pesi del modello per migliorare le prestazioni.
    \item \textbf{Modifiche all'Architettura}: Introduzione di nuove componenti o modifiche a quelle esistenti.
    \item \textbf{Integrazione di Dati Aggiuntivi}: Utilizzo di ulteriori dati di addestramento per migliorare le prestazioni.
\end{itemize}

\chapter{RAG}
\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.8\linewidth]{figures/RAGDefinizione.png}
    \label{fig:significa-RAG}
\end{figure}
\section{Introduzione}
La sigla RAG rappresenta \textbf{Retrieval Augmented Generation}, (in italiano \textit{Generazione Aumentata tramite Recupero} )  è un sistema che permette di migliorare l'output di un LLM estendendo la sua conoscenza con nuove informazioni, al di fuori dai suoi dati di addestramento allo scopo di:
\begin{itemize}
    \item ottenere risposte mirate e personalizzate contenti knowledge relativa a librerie e codice custom
    \item migliorare il codice generato rendendolo più specifico al dominio riducendo le allucinazioni
    \item facilitare l'assistenza da parte del modello nella fase di debugging migliorando la sua comprensione di sistemi complessi
    \item supportare la creazione di documentazione aggiornata
    \item permettere all'interno di un Team di migliorare la coerenza del codice scritto da diversi programmatori proponendo librerie e standard comuni
    \item evitare risposte imprecise a causa della confusione terminologica, in cui diverse fonti utilizzano la stessa terminologia per parlare di cose diverse.
\end{itemize}

\section{Funzionamento}
Il sistema RAG si integra al LLM attivando un meccanismo di recupero delle informazioni per aumentare il prompt della richiesta.
Il funzionamento si articola in diverse fasi qui sotto illustrate:
\newline
\begin{tikzpicture}[node distance=2cm]
    % Style definitions
    \tikzstyle{block} = [rectangle, draw, fill=white, 
        text width=3cm, text centered, rounded corners, minimum height=2.5em]
    \tikzstyle{line} = [draw, -latex', thick]
    \tikzstyle{data} = [text width=3cm, text centered]
    
    % Input nodes
    \node [block] (input) {Input\\(Query utente)};
    \node [block, right=of input] (docs) {Dataset locale\\(Es:File java)};
    
    % Processing nodes
  %  \node [block, below=of input] (query) {Query\\Processor};
    \node [block, below=of docs] (chunking) {chunking};
    
    % Embedding layer
    \node [block, below=of input] (qembed) {Embedding};
    \node [block, below=of chunking] (dembed) {Embedding};
    
    % Storage and retrieval
    \node [block, below=2.5cm of qembed] (vector) {Vector database};
    
    % LLM components
    \node [block, below=of vector] (context) {Top k similitary chunks};
    \node [block, right=of context] (llm) {LLM Prompt};
    \node [block, below=of context] (output) {LLM Answer};
    
    % Connections
    \path [line] (input) -- (qembed);
    \path [line] (docs) -- (chunking);
   % \path [line] (query) -- (qembed);
    \path [line] (chunking) -- (dembed);
    \path [line] (qembed) -- (vector);
    \path [line] (dembed) -- (vector);
    \path [line] (vector) -- (context);
    \path [line] (context) -- (llm);
    \path [line] (llm) -- (output);
    
    % Box around everything
    \node [draw, dashed, fit=(input) (docs) (chunking) (qembed) 
        (dembed) (vector) (context) (llm) (output),
        inner sep=1cm, label=above:Sistema RAG] {};
        
    % Add data flow labels
    \node [data] at ($(input)!0.5!(qembed)$) {Vettorizzazione};
    \node [data] at ($(chunking)!0.5!(dembed)$) {Vettorizzazione};
    \node [data] at ($(dembed)!0.5!(vector)$) {Crea};
    \node [data] at ($(vector)!0.5!(context)$) {Recupero};
    \node [data] at ($(context)!0.5!(llm)$) {Integra};
    \node [data] at ($(llm)!0.5!(output)$) {Genera};
\end{tikzpicture}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=.8\linewidth]{figures/jumpstart-fm-rag.jpg}
%    \caption{Flusso di una request ad un LLM integrato con un RAG}
%    \label{fig:jumpstart-fm-rag}
%\end{figure}

\subsection{Creazione Vector Database}
La propria \emph{knowledge base} deve essere salvata in un database vettoriale, in modo da poter essere interrogata in maniera efficiente dal sistema RAG.
Per creare questo database vengono utilizzati dati esterni al training set originale del LLM, provenienti da diverse fonti come:
\begin{itemize}
    \item API e database interni
    \item Archivi documentali
    \item File di testo e codice
\end{itemize}
La creazione di un database ben strutturato e \textbf{\underline{la parte più importante}} di tutto il processo, dividere il codice in chunk correttamente etichettando ogni elemento con i corretti metadati è fondamentale per la sucessiva fase di interrogazione.
Il processo di creazione del Vector Database segue la seguente pipeline:
\begin{itemize}
    \item \textbf{Chunking}: Divisione del codice in chunk 
    \item \textbf{Embedding}: Conversione dei chunk in vettori numerici
    \item \textbf{Vector Store}: Memorizzazione degli embedding in un database vettoriale
\end{itemize}
%Creazione degli Embedding


\subsection{Fase 1: User query e function calling}
Data la query d'input da parte dell'utente, il sistema RAG è avviato da una chiamata di funzione per ricercare nel
\textbf{Vector Database} i chunk più rilevanti per la query.
Nei modelli più complessi in RAG è di fatto un agente integrato nel sistema che viene chiamato all'occorrenza quando la base di conoscenza del LLM non è sufficiente per fornire una risposta adeguata,
in questo modo viene anche razionalizzato e ottimizzato il costo computazionale del processo,
attivato solo quando strettamente necessario.
Rimane comunque questo passaggio una scelta configurabile in base allo specifico utilizzo del sistema,
ad esempio per un azienda che utilizza il LLM solo per compiti specifici e sempre contestualizzati può essere configurato il sistema in modo che chiami
la funzione RAG sempre.
\subsection{Fase 2: Recupero delle Informazioni}
Quando l'utente sottopone una query:
\begin{itemize}
    \item La domanda viene convertita in un vettore
    \item Il sistema cerca nel database vettoriale le informazioni più pertinenti
    \item Viene calcolata la rilevanza attraverso calcoli matematici vettoriali
    \item Se trovate corrispondenze con uno score di similarità sufficiente, i chunk vengono recuperati
\end{itemize}

\begin{figure}[h]
    \raggedright
    \includegraphics[width=0.8\linewidth]{figures/plotSearch.png}
    \label{fig:Esempio distanze chunk e query}
\end{figure}
\subsection{Fase 3: Aumento del Prompt}
Se trovate, il sistema RAG arricchisce il prompt dell'utente con le informazioni recuperate, fornendo al LLM un contesto più ampio e dettagliato per generare una risposta coerente.

\section{Perchè RAG}
Il RAG permette di superare le limitazioni di conoscenza dei LLM, fornendo risposte accurate e contestualizzate 
grazie all'integrazione di conoscenze interne e personalizzate.
Dopo aver costruito un sistema RAG è possibile eseguire rapidamente aggiornamenti al \textbf{Vector database},
cosa che sarebbe molto più difficile da ottenere con il fine-tuning, che richiede tempo e risorse significative.
Avere un LLM addestrato fin da subito su misura per le proprie esigenze e sempre aggiornato sarebbe fantastico ma per quasi
tutte le aziende richiederebbe risorse impossibili da sostenere ed è quindi molto più comune costruire un RAG
che intervenire direttamente sulla conoscenza del LLM che restano solitamente proprietà di terzi.

\chapter{Implementazione di un sistema RAG per lo sviluppo di codice per il linguaggio Java}
\section{Obiettivo}
Questo caso studio si propone l'obiettivo di verificare il livello di personalizzazione e qualità delle risposte di un LLM integrato con un sistema RAG.
Potenziando la query nel prompt di input del LLM attraverso la creazione di un sistema RAG di supporto,
verranno costruite e analizzate singolarmente tutte le fasi che compongono il processo.
Il sistema RAG è stato testato con della \textbf{classi JAVA uniche} create appositamente per il caso studio.
\subsubsection{Problematica da affrontare:}
Chiamate a più livelli di classi e metodi, dove il RAG potrebbe non essere in grado di estrapolare
le informazione necessarie da inserire nel prompt per ottenere dal LLM risposte coerenti con quanto richiesto.

\section{Architettura del Sistema}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=2cm]
        % Components
        \node [block] (chunk) {Text Processor\\(Chunking)};
        \node [block, right=of chunk] (embed) {Embedder\\(BGE-M3)};
        \node [block, right=of embed] (db) {Vector DB\\(FAISS)};
        \node [block, below=of chunk] (retriever) {Retriever};
        \node [block, right=of retriever] (llm) {LLM Interface\\(Es:CodeQwen)};
        
        % Connections
        \path [line] (chunk) -- (embed);
        \path [line] (embed) -- (db);
        \path [line] (db) -- (retriever);
        \path [line] (retriever) -- (llm);
        
        % Box around everything
        \node [draw, dashed, fit=(chunk) (embed) (db) (retriever) (llm),
            inner sep=1cm] {};
    \end{tikzpicture}
    \caption{Architettura del sistema RAG}
    \label{fig:rag-architecture}
\end{figure}
Il sistema RAG implementa un'architettura modulare composta da cinque componenti principali:

\begin{enumerate}
    \item \textbf{Text Processor (Chunking)}:
    \begin{itemize}
        \item Suddivide i file Java in chunk di un numero definito appositamente di token
        \item Gestisce sovrapposizione di token tra chunk
        \item Preserva il contesto del codice
    \end{itemize}

    \item \textbf{Embedder (BGE-M3)}:
    \begin{itemize}
        \item Converte i chunk in vettori numerici
        \item Utilizza il modello BGE-M3 per la generazione degli embedding
        \item Normalizza i vettori per ottimizzare la ricerca
    \end{itemize}

    \item \textbf{Vector DB (FAISS)}:
    \begin{itemize}
        \item Memorizza gli embedding in un database vettoriale
        \item Ottimizza la ricerca per similarità
        \item Garantisce recupero efficiente dei chunk rilevanti
    \end{itemize}

    \item \textbf{Retriever}:
    \begin{itemize}
        \item Esegue query semantiche sul database
        \item Recupera i k chunk più rilevanti
        \item Prepara il contesto per il LLM
    \end{itemize}

    \item \textbf{LLM Interface (CodeQwen e Llama 3.2)}:
    \begin{itemize}
        \item Interfaccia tramite Ollama con i LLM CodeQwen e Llama 3.2
        \item Genera risposte basate sul contesto recuperato
    \end{itemize}
\end{enumerate}

\section{Software Utilizzati}
\subsection{Ollama\hspace{0.3cm}\protect\includegraphics[width=0.03\linewidth]{figures/ollama.png}}
Ollama \cite{ollama-docs} è un software che permette di utilizzare in locale LLM
senza dover dipendere da servizi cloud esterni.
Il software è stato scelto per la sua flessibilità, permettendo di integrare facilmente i modelli LLM nel sistema RAG.
\subsubsection{LLM}
Ogni LLM è specializzato per determinati scopi,
per questo motivo per rendere più completa la ricerca sono stati utilizzati due modelli con caratteristiche differenti:
\subsection{Llama 3.2}
Llama 3.2 3B \cite{llama3-2}, un modello di linguaggio open source.
Il modello, con 3 miliardi di parametri, è ottimizzato per compiti di dialogo multilingue e si distingue per le sue capacità di recupero e sintesi delle informazioni.
La scelta è ricaduta su questa versione per il suo equilibrio tra prestazioni e requisiti computazionali che permottono il suo utilizzo senza hardware troppo potente.

\subsection{Codeqwen 1.5}
Codeqwen \cite{codeqwen1.5} è un modello di linguaggio open source specializzato nella generazione di codice e documentazione tecnica.  
Con 7 miliardi di parametri, il modello è stato addestrato su un ampio dataset di codice sorgente e documentazione tecnica, permettendo di generare codice coerente e ben strutturato.
La scelta di questo modello è stata dettata, a differenza di llama3.2, dalla sua specializzazione nella programmazione e dalla sua capacità di generare codice di alta qualità. 
%\subsubsection{qwen2.5-coder:3b}
%qwen2.5-coder \cite{qwen-coder} è stato sviluppato da Qwen AI ed è anchesso open source, specializzato nella generazione di codice e documentazione tecnica.
%Con 3 miliardi di parametri, il modello è stato addestrato su un ampio dataset di codice sorgente e documentazione tecnica, permettendo di generare codice coerente e ben strutturato. 
%La scelta di questo modello è stata dettata, a differenza di llama3.2, dalla sua specializzazione nella programmazione e dalla sua capacità di generare codice di alta qualità.

\subsection{LangChain}
LangChain \cite{langchain} è un framework open source progettato per costruire applicazioni basate su LLM.
Fornisce strumenti avanzati per integrare modelli con dati esterni ed API, creare pipeline con chain
e gestire database vettoriali, supportando l'implementazione di sistemi RAG.

\subsection{BGE-M3}
BGE-M3 \cite{bge-m3} è un modello di embedding testuale open source per la gestione di dati strutturati e non strutturati multilingue.
Il modello permettendo di convertire testo in vettori numerici ad alta dimensionalità.

\subsection{FAISS}
FAISS (Facebook AI Similarity Search) \cite{faiss} è una libreria open source per la ricerca efficiente di similarità e il clustering di vettori densi.
Progettata per gestire dataset su larga scala, FAISS supporta operazioni di ricerca anche su insiemi di vettori che superano la capacità della RAM, grazie a tecniche di indicizzazione avanzate e ottimizzazioni computazionali.

\section{Dataset}
Il dataset è stato creato appositamente per testare il sistema RAG ed è composto da diciannove classi Java:

\begin{description}
    \item[DateUtilCustom.java] Classe personalizzata per gestire le date
    \item[GiorniMagici.java] Classe per calcolare in maniera particolare dei giorni
    \item[BasketballStats.java] Classe abstract per statistiche di basket
    \item[AdvancedBasketballStats] Classe che estende BasketballStats
    \item[BasketballTest] Classe per testare le statistiche di basket implementate in AdvancedBasketballStats
    \item[Altre classi java] Non strettamente correlate con le prime due utili per aumentare la base dati sul quale effettuare le ricerche e per testare la capacità di generalizzazione del sistema.
\end{description}

\section{Scenario base del Caso Studio}
DateUtilCustom.java e GiorniMagici.java sono strettamente correlate infatti GiorniMagici.java richiama metodi definiti in DateUtilCustom.java.
Andremo a testare il sistema RAG con la seguente query: 
\begin{itemize}
    \item \textbf{Cosa ritorna il metodo \texttt{segnaleWow(LocalDate.of(2025, 2, 14))}?}
\end{itemize}

\subsection{Codice di riferimento per rispondere alla query}
In GiorniMagici.java è presente la seguente funzione:
\begin{lstlisting}[language=Java, caption={Metodo segnaleWow in GiorniMagici.java}, label={lst:segnaleWow}]
public static String segnaleWow(LocalDate data) {
    String wow = "il tuo segnale Wow e': " + DateUtilCustom.getMessaggioMagico(date);
    return wow;
}
\end{lstlisting}
Questa funzione richiama il metodo \texttt{getMessaggioMagico} presente in DateUtilCustom.java:

\begin{lstlisting}[language=Java, caption={Metodo getMessaggioMagico in DateUtilCustom.java}, label={lst:getMessaggioMagico}]
public static String getMessaggioMagico(LocalDate datamagica) throws DateTimeParseException {
    DayOfWeek giornoSettimana = datamagica.getDayOfWeek();
    switch(giornoSettimana) {
        case MONDAY: return "La magia inizia nel silenzio...";
        case TUESDAY: return "I sussurri degli antichi si fanno sentire.";
        case WEDNESDAY: return "Il velo tra i mondi e' sottile oggi.";
        case THURSDAY: return "L'energia magica e' potente e chiara.";
        case FRIDAY: return "Attenzione agli incantesimi del crepuscolo.";
        case SATURDAY: return "Il giorno perfetto per scoprire segreti nascosti.";
        case SUNDAY: return "Riposa e rigenera il tuo potere magico.";
        default: return "Il giorno e' avvolto nel mistero...";
    }
}
\end{lstlisting}

\subsection{Risultato Atteso}
Essendo il 14 Febbraio 2025 un venerdì, ci aspettiamo come risposta:
\begin{quote}
    \textbf{``il tuo segnale Wow è: Attenzione agli incantesimi del crepuscolo.''}
\end{quote}

\section{Implementazione}
\subsection{Creazione dei Chunk}
I modelli di embedding hanno limiti massimi di input (512-4096 token)  per questo spezzare il codice in chunk di dimensioni adeguate è obbligatorio oltre ad essere in ogni caso fondamentale.
Inoltre occorre prestare attenzione alla dimensione dei chunk generati, se troppo piccoli riducono il contesto disponibile per il modello mentre se troppo grandi perdono focalizzazione semantica.
Per suddividere il file Java in chunk viene utilizzata la libreria \textbf{langchain\_text\_splitters.}
Il seguente codice Python mostra come suddividere i file Java in chunk di dimensione fissa, salvando i risultati in un file JSON.
\begin{lstlisting}[language=Python, caption={Codice Python per la suddivisione dei file Java in chunk}, label={lst:chunking}, inputencoding=utf8]
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    import json
    
    # Funzione per caricare e suddividere un file Java
    def process_file(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
         lines = f.readlines()
    
        # Ricostruisce il testo mantenendo le informazioni sulle linee
        text = ''.join(lines)
    
        splitter = RecursiveCharacterTextSplitter(
        chunk_size=512, # medio-basso per prevenire merge di metodi
        chunk_overlap=128,
        separators=[
            # I seguenti separatori sono stati usati per mantenere i metodi uniti
            # Prioritari: catturano la fine dei metodi
            "\n}\n\npublic ",
            "\n}\n\nprivate ",
            "\n}\n\nprotected ",
            "\n}\n\nstatic ",
            "\n}\n\n// End of method", 
            
            # Secondari: separatori generici
            "\nclass ",  # Inizio nuove classi
            "\n@",  # Annotazioni
            "\n/**",  # Javadoc
            "\n * ", 
            "\n"
        ],
        keep_separator=True
        is_separator_regex=False
    )
    
        chunks = splitter.split_text(text)
        # Calcola le linee esatte per ogni chunk
        chunk_metadata = []
        cursor = 0
        for chunk in chunks:
            start_line = text.count('\n', 0, cursor) + 1
            chunk_length = len(chunk)
            end_line = text.count('\n', 0, cursor + chunk_length) + 1
            chunk_metadata.append({
                "start_line": start_line,
                "end_line": end_line,
                "text": chunk
            })
            cursor += chunk_length
        
        return chunk_metadata
    
    # Carica e suddividi i file Java
    files = glob.glob("my_project/classi_java_custom/*.java") #["my_project/DateUtilCustom.java", "my_project/GiorniMagici.java", "my_project/BasketballStats.java", "my_project/Ventunoclassi.java"]
    all_chunks = []
    
    for file_path in files:
        chunks_info = process_file(file_path)
        for chunk_info in chunks_info:
            chunk_text = chunk_info["text"]
            
            # Aggiungi contesto strutturale
            class_context = ""
            if "class " in chunk_text:
                class_name = chunk_text.split("class ")[1].split("{")[0].strip()
                class_context = f"Classe: {class_name}\n"
            
            all_chunks.append({
                "id": len(all_chunks) + 1,
                "text": f"// File: {file_path}\n{class_context}{chunk_text}",
                "source": file_path,
                "type": "code",
                "start_line": chunk_info["start_line"],
                "end_line": chunk_info["end_line"],
                "class": class_context.replace("Classe: ", "") if class_context else ""
            })
    
    # Salva i chunk in un file JSON
    with open("chunks.json", "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, indent=4, ensure_ascii=False)
\end{lstlisting}
Il chunking è costruito in maniera specifica per \textbf{codice java}, i separatori sono stati scelti per tentare di segmentare il codice secondo la struttura tipica dei metodi e delle classi, garantendo che il chunk contenga blocchi di codice "interi".
L'opzione \texttt{keep\_separator=False} crea punti di split più naturali per il codice Java, allineandosi meglio con la struttura dei metodi e delle classi.
Per ciascun chunk, se nel testo è presente la stringa \texttt{class}, il codice estrae il nome della classe 
(prendendo il testo che segue \texttt{class} fino al primo \texttt{\{}) e lo utilizza per creare un contesto 
strutturale (es \texttt{Classe: NomeClasse}).
Questo contesto viene preappeso al testo del chunk e salvato anche come valore nel campo "class".

\subsubsection{Il risultato nel file chunks.json è il seguente:}
    \begin{lstlisting}[language=json,firstnumber=1, caption={Esempio di chunks generati}, label={lst:chunks-example}]
        [
            {
                "id": 1,
                "text": "// File: my_project/classi_java_custom\\AdvancedBasketballStats.java\nClasse: AdvancedBasketballStats extends BasketballStats\npackage classi_java_custom;\nimport java.time.LocalDate;\nimport java.util.Arrays;\npublic class AdvancedBasketballStats extends BasketballStats {\n    \n    // Override con formula avanzata PER\n    @Override\n    public double calcolaEfficienzaGiocatore(int punti, int rimbalzi, int assist, \n                                           int pallePerse, int tiriSbagliati) {\n        return (punti * 1.2 + rimbalzi * 1.1 + assist * 1.3) - \n              (pallePerse * 0.9 + tiriSbagliati * 0.7);\n    }",
                "source": "my_project/classi_java_custom\\AdvancedBasketballStats.java",
                "type": "code",
                "start_line": 1,
                "end_line": 14,
                "class": "AdvancedBasketballStats extends BasketballStats\n"
            },
            {
                "id": 2,
                "text": "// File: my_project/classi_java_custom\\AdvancedBasketballStats.java\n(pallePerse * 0.9 + tiriSbagliati * 0.7);\n    }\n    \n    // Override con valutazione dettagliata\n    @Override\n    public String valutaTiro(double distanzaCanestro, int pressioneDifensiva) {\n        if (distanzaCanestro < 1.5 && pressioneDifensiva < 4) {\n            return \" Tiro ad alto rendimento (85% successo) \";\n        } else if (distanzaCanestro >= 7.0 && pressioneDifensiva > 6) {\n            return \" Tiro ad alto rischio (30% successo) \";\n        }",
                "source": "my_project/classi_java_custom\\AdvancedBasketballStats.java",
                "type": "code",
                "start_line": 14,
                "end_line": 24,
                "class": ""
            },
            ............continua
        ]
    \end{lstlisting}
        
    Ogni chunk mantiene:
    \begin{itemize}
        \item Il riferimento al file sorgente
        \item Il nome della classe
        \item Le righe di inizio e fine nel file originale
        \item Il contenuto del codice con la sua struttura
    \end{itemize}

\subsection{Arricchire i chunk con metadati relativi al codice}
    Oltre al testo del codice, è importante mantenere informazioni aggiuntive per facilitare la ricerca e l'interpretazione dei chunk.
    La seguente funzione \texttt{extract\_method\_name} aggiunge una stringa contestuale per ogni chunk che include:
    \begin{itemize}
        \item Il nome del metodo o della classe
        \item La classe di appartenenza
        \item Le righe di inizio e fine del codice
    \end{itemize}

    \begin{lstlisting}[language=Python, caption={Funzione extract\_method\_name}]
    import re
    def extract_method_name(text):
        # Pattern per la firma di un metodo in Java
        method_pattern = r'(?:public|private|protected|static|final|synchronized|abstract|native)\s+[\w<>\[\]]+\s+(\w+)\s*\([^)]*\)'
        
        # Pattern per i costruttori
        constructor_pattern = r'(?:public|private|protected)\s+(\w+)\s*\([^)]*\)'
        
        # Cerca la firma di un metodo
        matches = re.findall(method_pattern, text)
        if matches:
            return matches[0]  # Restituisce il primo metodo trovato
        
        # Cerca costruttori
        constr_matches = re.findall(constructor_pattern, text)
        if constr_matches:
            return constr_matches[0] + " (costruttore)"
        
        # Cerca chiamate a metodi
        method_calls = re.findall(r'\.(\w+)\s*\(', text)
        if method_calls:
            return f"Chiamata a: {method_calls[-1]}"
        
        return "unknown_method"  # Default se non trova nulla
    \end{lstlisting}
        
\subsection{Generazione degli Embedding}
    Gli embedding trasformano i chunk in rappresentazioni vettoriali che catturano il significato semantico.
    Il seguente codice Python mostra come generare gli embedding e creare un database Faiss.
    Come precedentemente descritto, il modello di embedding utilizzato è BGE-M3, questo modello usa due rappresentazioni per complementarietà, la rappresentazione densa cattura relazioni semantiche mentre quella sparsa cattura relazioni sintattiche.
    Mentre sul database FAISS ad alta dimensionalità verrà settata la ricerca di somiglianza utilizzando la distanza euclidea tra i vettori.
    
    \begin{lstlisting}[language=Python, caption={Codice Python per la generazione degli embedding e la creazione di un database FAISS}, label={lst:embeddings}]
        import json
        from sentence_transformers import SentenceTransformer
        from langchain_community.vectorstores import FAISS
        
        # 1. Carica i chunk dal file JSON
        with open("chunks.json", "r", encoding="utf-8") as f:
            chunks_data = json.load(f)
        
        chunks = [item["text"] for item in chunks_data]
        
        # 2. Carica il modello BGE-M3 e genera gli embedding
        embedder = SentenceTransformer('BAAI/bge-m3')
        embeddings = embedder.encode(
            [f"METHOD:{extract_method_name(c['text'])} CLASS:{c['class']} LINES:{c['start_line']}-{c['end_line']} CONTENT:{c['text']}" 
             for c in chunks_data],
            show_progress_bar=True
        )
        
        # 3. Crea un database FAISS
        vector_store = FAISS.from_embeddings(
            text_embeddings=list(zip(chunks, embeddings)),  # Abbina testi e embedding
            embedding=embedder,  # Modello per future operazioni
        )
        
        # 4. Salva il database
        vector_store.save_local("./faiss_db")
        print("Database FAISS creato e salvato in ./faiss_db.")
    \end{lstlisting}

Il metodo \emph{encode()} del modello BGE-M3 genera gli embedding per ogni chunk,
chiamando la funzione \emph{extract\_method\_name} per arricchire il contesto e creare vettori con relazioni semantiche strutturate.

\subsection{Esecuzione di query sul Database FAISS}
    Una volta creato il database FAISS, è possibile eseguire ricerche semantiche sui chunk memorizzati:

    \begin{lstlisting}[language=Python, caption={Esecuzione di una query sul database FAISS}, label={lst:query}]
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings

    # 1. Carica il modello di embedding nel formato corretto
    embedder = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 2. Carica il database FAISS esistente
    vector_store = FAISS.load_local(
        folder_path="./faiss_db",
        embeddings=embedder,
        allow_dangerous_deserialization=True
    )

    # 3. Query di esempio
    query = "Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 1, 10))?"

    # 4. Cerca i chunk piu' simili
    docs = vector_store.similarity_search_with_score(
        query,
        k=5,
        score_threshold=0.90,  # bassa similarita'
        search_type="similarity",  # Piu' efficace per il codice
        lambda_mult=0.5       # Bilancia diversita'/rilevanza
    )

    # 5. Stampa i risultati con relativo score
    for i, (doc, score) in enumerate(docs):
        print(f"Risultato {i+1} (Score: {score:.4f}):")
        print(doc.page_content)
        print("-" * 40)
    \end{lstlisting}

    \subsubsection{Risultati con query base (senza alcun riferimento al metodo utilizzato all'interno di segnale Wow)}
    \begin{itemize}
        \item \textbf{Query:} 
            \newline
                \textbf{``Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 1, 10))?''}
            \newline
        \item \textbf{Output:}
        viene restituito il chunk corretto con uno score di similarità di \textbf{0.7014}. Questo valore, basato sulla cosine similarity, non è particolarmente alto ma sufficiente per identificare il chunk corretto.
    \end{itemize}
    \paragraph{Nota:}
    È importante riscontrare che viene restituito un solo chunk nonostante \texttt{k=5}.
    Questo accade perché nessun altro chunk supera la soglia di similarità impostata.
    Tale comportamento evidenzia una criticità: la funzione \texttt{segnaleWow()} richiama un metodo presente nella libreria \texttt{DateUtilCustom} che non viene estratto dal Dataset.

    \subsubsection{Riformulazione query (aggiungendo riferimento al metodo utilizzato all'interno di segnale Wow)}
        \begin{itemize}
            \item \textbf{Per risolvere questo problema, la query è stata riformulata:}
            \begin{quote}
                \textbf{``Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 1, 10)) che utilizza la funzione getMessaggioMagico() della libreria DateUtilCustom?''}
            \end{quote}
            \item \textbf{L'output fornisce 5 risultati:}
            \begin{itemize}
                \item Primo chunk \textbf{(score: 0.5441)}: contiene la funzione \texttt{segnaleWow}
                \item Secondo, terzo e quarto chunk \textbf{(scores: 0.7325, 0.7466, 0.7989)}: contengono la funzione \texttt{getMessaggioMagico}
                \item Quinto chunk \textbf{(score: 0.8301)}: funzione non rilevante relativa alle date
            \end{itemize}
        \end{itemize}

    \paragraph{Conclusione:}
    Sono state riscontrate due problematiche molto rilevanti, la prima riguarda la mancanza di estrazione di metodi da librerie esterne se non esplicitate nella query.
    Mentre la seconda guarda i chunk estratti, lo score ottenuto non è particolarmente alto e questo con un database più ampio relativo a funzioni con terminologie e meccanismi simili potrebbe portare a risultati non coerenti.
    Per il secondo punto questa analisi ha portato alla decisione di abbassare \texttt{score\_threshold} da 0.90 a 0.80,
    questa piccola correzione risolve in parte la problematica o almeno evita di propagarla ulteriormente preferendo non ottenere risultati piuttosto che ricevere risposte non coerenti.
    
\subsection{Creazione della Pipeline RAG}
\begin{lstlisting}[language=Python, caption={Pipeline RAG}, label={lst:rag}]
    from langchain_community.vectorstores import FAISS
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_ollama import OllamaLLM
    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain.prompts import PromptTemplate
    
    # Configurazione embedding
    embedder = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Caricamento database FAISS
    vector_store = FAISS.load_local(
        folder_path="./faiss_db",
        embeddings=embedder,
        allow_dangerous_deserialization=True
    )
    # Aggiunta del database FAISS al retriever
    retriever=vector_store.as_retriever(
            search_kwargs={
                "k": 5,                   # Piu' documenti per contesto
                "score_threshold": 0.80, # medio-bassa similarita' inizialmente era 0.90
                "search_type" :"similarity",  # Piu' efficace per il codice
                "lambda_mult":0.5       # Bilancia diversita'/rilevanza
            }
        )
    
    varStileLLM = "Sei un programmatore che risponde conciso e sintetico."
    
    # Configurazione Template del prompt specifici per i modelli
    LLAMA_TEMPLATE = """<|begin_of_text|>
    <|start_header_id|>system""" + varStileLLM + """<|end_header_id|>
    Contesto: {context}<|eot_id|>
    <|start_header_id|>user<|end_header_id|>
    Domanda: {input}<|eot_id|>
    <|start_header_id|>assistant<|end_header_id|>"""
    
    CODEQWEN_TEMPLATE = """<|im_start|>system """ + varStileLLM + """
    {context}<|im_end|>
    {{ if .Functions }}<|im_start|>functions
    {{ .Functions }}<|im_end|>{{ end }}
    <|im_start|>user
    {input}<|im_end|>
    <|im_start|>assistant
    """
    
    COMMON_PARAMS = {
        "temperature": 0.3, #lasciamo una bassa creativita' non vogliamo che inventi risposte
        "top_p": 0.85  # Bilancia creativita'/controllo nei token generati
    }
    
    # Caricamento modello
    def load_model(model_name):
        models = {
            "llama3.2": {
                "template": LLAMA_TEMPLATE,
                "params": COMMON_PARAMS
            },
            "codeqwen": {
                "template": CODEQWEN_TEMPLATE,
                "params": COMMON_PARAMS
            }
        }
        if model_name not in models:
            raise ValueError(f"Modello non supportato: {model_name}")
        return OllamaLLM(
            model=model_name,
            **models[model_name]["params"]
        ), PromptTemplate(
            template=models[model_name]["template"],
            input_variables=["input", "context"]
        )
    
    # Inizializza il modello
    llm, prompt = load_model("codeqwen")
    
    # Catena RAG
    document_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(
        retriever,
        document_chain
    )
    
    # Funzione query
    def ask_ollama(question):
        try:
            result = rag_chain.invoke({"input": question})
            print("DOMANDA:", question)
            print("RISPOSTA:")
            print(result["answer"])
            print("FONTI:")
            for i, doc in enumerate(result["context"], 1):
                print(f"{i}. {doc.page_content[:150]}...")
                if 'source' in doc.metadata:
                    print(f" Fonte: {doc.metadata['source']}")
                print("-" * 80)
        except Exception as e:
            print(f"ERRORE: {str(e)}")
    
    # Esempio d'uso
    if __name__ == "__main__":
        ask_ollama("Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 2, 14)) che utilizza la funzione getMessaggioMagico() della libreria DateUtilCustom?")
        #ask_ollama("Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 2, 14))?")
\end{lstlisting}
\subsubsection{Spiegazione Pipeline del RAG}
    Seguendo la struttura precedentemente creata, per eseguire l'embedder della query di input viene utilizzato il modello BAAI/bge-m3 e
    caricato il database FAISS contenente la \textbf{knowledge base}.
    La chiamata iniziale alla funzione \emph{ask\_ollama()} richiede come parametro \textbf{la query di input} per poi essere processata dalla pipeline RAG.
    Sfruttando le funzionalità della libreria LangChain \cite{langchain-retrieval-chain}, \textbf{\emph{result}} sarà un array contente la risposta("answer") e il contesto("context") fornito alla query.
\subsubsection{\emph{rag\_chain.invoke()}}
 Questa funzione esegue la catena RAG creata tramite il metodo \emph{create\_retrieval\_chain()} che prende come parametri il retriever e il document chain.
 \begin{itemize}
    \item la funzione \textbf{create\_stuff\_documents\_chain()} carica una catena di documenti prendendo in input il modello LLM e il template del prompt.
    \item \textbf{load\_model()} carica il modello LLM e il template del prompt in base al modello scelto sfruttando OllamaLLM e PromptTemplate.
 \end{itemize}
 \subsubsection{Temperature}
Per i due LLM è stata data una temperature molto bassa \textbf{0.3} in modo da garantire da parte dei LLM risposte coerenti e precise senza provi ad inventarle.
\subsubsection{Top\_p}
Il parametro \texttt{top\_p} è stato impostato a 0.85 per bilanciare creatività e controllo nei token generati.
\subsubsection{System}
Come parametro di sistema da passare al LLM è stata creata la variabile \texttt{varStileLLM} con il seguente valore: \textbf{"Sei un programmatore che risponde conciso e sintetico"}
questa richiesta verrà sempre passata al LLM e condizionerà tutte le risposte fornite.

\section{Test Sistema RAG}
Valutiamo se il sistema RAG  è in grado di rispondere in maniera coerente alla query proposta.
Ricordiamo che il risultato atteso è: \textbf{``il tuo segnale Wow è: Attenzione agli incantesimi del crepuscolo.''}    
\subsection{Query base senza riferimenti al metodo utilizzato all'interno di segnaleWow}
\begin{quote}
    \texttt{``Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 2, 14))?''}
\end{quote}
\subsubsection{Fonti aggiunte al prompt}
Il RAG aggiunge solo il chunk dove è presente la funzione \texttt{segnaleWow} e non il chunk con la funzione \texttt{getMessaggioMagico} della libreria DateUtilCustom
per questo le risposte dei due modelli sono incomplete:
\subsubsection{Output con LLM Llama3.2}
L'output ottenuto utilizzando il modello Llama3.2 è stato:
\begin{quote}
    ``Il metodo \texttt{segnaleWow} restituisce una stringa che contiene un messaggio magico associato alla data specificata. In questo caso, la data è il 14 Febbraio 2025.
La risposta esatta sarebbe:
"il tuo segnale Wow è : [messaggio magico]"
dove [messaggio magico] è il risultato dell'applicazione di `DateUtilCustom.getMessaggioMagico(date).''
\end{quote}

\subsubsection{Modifica Temperature}
CodeQwen provava a dare risposta anche senza avere tutte le informazioni necessarie, ho deciso di abbassare la temperatura da 0.3 a 0.2 per vedere se il modello CodeQwen avrebbe risposto in maniera più precisa.
Cosa che è avvenuta:

\subsubsection{Output con LLM CodeQwen}
Utilizzando invece il modello CodeQwen, la risposta ottenuta è stata:
\begin{quote}
    ``Se si chiama il metodo `segnaleWow` con un'istanza di `LocalDate` che rappresenta la data "14 febbraio 2025", il valore restituito sarà una stringa che indica il segnale Wow associato a tale data.
    ```java
    String result = GiorniMagici.segnaleWow(LocalDate.of(2025, 2, 14));
    System.out.println(result);
    ```
    Il metodo `getMessaggioMagico` viene richiamato su un'istanza di `DateUtilCustom`, passando la data "14 febbraio 2025" come parametro. Questo metodo presumibilmente restituisce un messaggio magico associato a tale data, ad esempio "La data del New Year è una data speciale".
    Il risultato finale sarà:
    ```
    il tuo segnale Wow è : La data del New Year è una data speciale``
\end{quote}

\subsection{Query Completa con riferimenti al metodo utilizzato all'interno di segnaleWow}
\begin{quote}
    \texttt{``Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 2, 14)) che utilizza la funzione getMessaggioMagico() della libreria DateUtilCustom?''}
\end{quote}
\subsubsection{Fonti aggiunte dal RAG al prompt}
In questo caso il RAG aggiunge 5 chunk, ovvero oltre a quello relativo alla funzione \texttt{segnaleWow} anche i chunk relativi alla funzione \texttt{getMessaggioMagico} della libreria DateUtilCustom.
Ne consegue che \textbf{le risposte date dai due modelli sono coerenti con quanto atteso}.
\subsubsection{Output con LLM Llama3.2}
``Il metodo \texttt{segnaleWow} restituisce una stringa che contiene il messaggio magico per la data specificata. 
Per determinare il messaggio esatto, dobbiamo analizzare la data fornita: `LocalDate.of(2025, 2, 14)`.
La data è il 14 febbraio 2025. Il giorno della settimana corrisponde a `FRIDAY`.
Quindi, il messaggio magico restituito dal metodo `getMessaggioMagico` sarà:
\textbf{"Attenzione agli incantesimi del crepuscolo."}''
\subsubsection{Output con LLM CodeQwen}
``Se si chiama il metodo `segnaleWow` con una data di `LocalDate.of(2025, 2, 14)`, utilizzando la funzione `getMessaggioMagico()` della libreria `DateUtilCustom`, il risultato sarà:
```
Il tuo segnale Wow è : \textbf{Il giorno perfetto per scoprire segreti nascosti.}
```
Questo è dato che la data 2025-02-14 cade mercoledì, quindi il metodo `getMessaggioMagico()` restituisce il messaggio "Il giorno perfetto per scoprire segreti nascosti.''
\subsection{Commento risultati ottenuti}
I risultati ottenuti mostrano come il sistema RAG sia in grado di rispondere in maniera coerente alla query proposta.
La ricerca dei chunk più simili funziona correttamente soprattutto se si scrive la richiesta in maniera più dettagliata possibile.
CodeQwen ha sbagliato a calcolare il giorno della settimana a causa di un suo errore nel calcolare il giorno della settimana corrispondente alla data fornita come parametro ma la risposta utilizza correttamente il metodo getMessaggioMagico().

\section{Valutazione ``llm as a judge"}
Per eseguire una valutazione più ampia del sistema usiamo l'approccio \textbf{\emph{"llm as a judge"}} per valutare automaticamente quanto prodotto dal sistema RAG.
Generiamo \textbf{30 domande} sulle quali sarà richiesta risposta al sistema RAG e sucessivamente eseguita una valutazione automatizzata delle risposte prodotte da parte di un altro LLM.
\subsection{Crezione domande}
Passando un file contenente tutte le librerie a \textbf{NotebookLM}, sono state generate 30 domande per valutare il sistema RAG.
La domanda fatta al LLM è stata:
\begin{quote}
    \texttt{``Dalle mie classi genera 30 domande/risposte per valutare il mio rag la prima è:
    \newline Cosa ritorna il metodo segnaleWow(LocalDate.of(2025, 2, 14))
    che utilizza la funzione getMessaggioMagico() della libreria
    DateUtilCustom?''}
\end{quote}
Il risulta è stato il seguente:
\begin{lstlisting}[language=json, caption={Domande/Risposte generate da NotebookLM}]
    {
        "question1": "Cosa ritorna il metodo `segnaleWow(LocalDate.of(2025, 2, 14))` che utilizza la funzione `getMessaggioMagico()` della libreria `DateUtilCustom`?",
        "answer": "Ritorna la stringa \"il tuo segnale Wow e': Attenzione agli incantesimi del crepuscolo.\""
    },
    {
        "question2": "La classe `AnalizzatoreRilascio` contiene un metodo chiamato `stimaDataRilascio`. Quali sono i due parametri di input richiesti da questo metodo?",
        "answer": "Il metodo `stimaDataRilascio` richiede un array di interi (`int[] taskCompletati`) e un valore double (`double velocitaSviluppo`) come input."
    },
    {
        "question3": "Nella classe `AnalizzatoreRilascio`, come viene calcolato il numero di task rimanenti nel metodo `stimaDataRilascio`?",
        "answer": "Il numero di task rimanenti viene calcolato sottraendo la somma dei task completati (assumendo un totale di 100 task)."
      },
    {
        "question4": "Qual e' il valore di soglia critica per l'indice di rischio rilascio (IRR) nella classe `AnalizzatoreRilascio`?",
        "answer": "La soglia critica per l'IRR e' **8.5**."
      },
    {
        "question5": "Quali sono i parametri di input del metodo `calcolaProbabilitaSuccesso` nella classe `AnalizzatoreRilascio` e come viene calcolata la probabilita' di successo?",
        "answer": "I parametri sono `numeroBugRisolti` e `numeroTestPassati`. La probabilita' di successo e' calcolata come `(numeroBugRisolti * 0.4) + (numeroTestPassati * 0.6)`."
    },
    {
        "question6": "Qual e' lo scopo del metodo `stimaTempoRimanente` nella classe `AnalizzatoreRilascio` e quali parametri accetta?",
        "answer": "Stima il tempo rimanente per il rilascio. Accetta `taskRimanenti` (numero di task rimanenti) e `velocitaSviluppo` come parametri."
    },
    {
        "question7": "Nella classe `BasketballStats`, qual e' la formula utilizzata per calcolare l'efficienza di un giocatore nel metodo `calcolaEfficienzaGiocatore`?",
        "answer": "La formula e' `(punti + rimbalzi + assist) - (pallePerse + tiriSbagliati)`."
    },
    {
        "question8": "Quali sono i criteri utilizzati nel metodo `valutaTiro` della classe `BasketballStats` per valutare la qualita' di un tiro?",
        "answer": "La **distanza dal canestro** (`distanzaCanestro`) e la **pressione difensiva** (`pressioneDifensiva`)."
    },
    {
        "question9": "Cosa restituisce il metodo `analisiStatisticheSquadra` della classe `BasketballStats`?",
        "answer": "Restituisce un report dettagliato delle prestazioni della squadra."
    },
    {
        "question10": "Quali sono gli input del metodo `previsioneVincitore` nella classe `BasketballStats` e cosa rappresentano?",
        "answer": "Gli input sono due array di double (`squadraCasa` e `squadraOspite`) che contengono `[puntiMedi, rimbalziMedi, assistMedi]`, e una `LocalDate dataPartita`."
    },
    {
        "question11": "Come viene calcolato il margine nel metodo `previsioneVincitore` della classe `BasketballStats`?",
        "answer": "Il margine e' calcolato come il valore assoluto della differenza tra la valutazione della squadra di casa e la valutazione della squadra ospite."
    },
    {
        "question12": "Cosa rappresenta l'input del metodo `calcolaMomentum` nella classe `BasketballStats`?",
        "answer": "Rappresenta gli ultimi punti segnati dalla squadra di casa (`ultimiPuntiCasa`) e dalla squadra ospite (`ultimiPuntiOspite`)."
    },
    {
        "question13": "Qual e' il requisito per la lunghezza della chiave segreta nel metodo `cifraturaVortex` della classe `CifratoreAziendale`?",
        "answer": "La chiave segreta deve essere di 12 caratteri."
    },
    {
        "question14": "Cosa fa il metodo `mascheraDatiSensibili` nella classe `TrasformatoreDati`?",
        "answer": "Il metodo maschera i dati sensibili sostituendo i caratteri originali con caratteri casuali."
    },
    {
        "question15": "Nella classe `DateUtilCustom`, descrivi cosa fa il metodo `formatDate(Date date)` e fornisci un esempio del formato di output.",
        "answer": "Formatta una data nel formato \"dd/MM/yyyy\". Ad esempio, se la data e' il 25 dicembre 2023, l'output sara' \"25/12/2023\"."
    },
    {
        "question16": "Qual e' la differenza tra i metodi `formatDate(Date date)` e `formatDate(Date date, String format)` nella classe `DateUtilCustom`?",
        "answer": "Il primo utilizza il formato predefinito \"dd/MM/yyyy\", mentre il secondo permette di specificare un formato personalizzato."
    },
    {
        "question17": "Cosa puo' accadere se si utilizza il metodo `parseDate` della classe `DateUtilCustom` con una stringa che non corrisponde al formato specificato?",
        "answer": "Verra' lanciata una `ParseException`."
    },
    {
        "question18": "Nella classe `DateUtilCustom`, cosa calcola il metodo `getDifferenceInDays(Date startDate, Date endDate)`?",
        "answer": "Calcola la differenza in giorni tra due date."
    },
    {
        "question19": "Cosa fa il metodo `addDays(Date date, int days)` nella classe `DateUtilCustom`?",
        "answer": "Aggiunge un numero specificato di giorni a una data."
    },
    {
        "question20": "Come si verifica se una data e' nel passato usando la classe `DateUtilCustom`?",
        "answer": "Utilizzando il metodo `isPastDate(Date date)`."
    },
    {
        "question21": "Cosa fa il metodo `giorniAlmiocompleannoSpecial(Date dataNascita, String nome)` nella classe `DateUtilCustom`?",
        "answer": "Calcola i giorni che mancano al compleanno di una persona e restituisce un messaggio personalizzato."
    },
    {
        "question22": "In `ElaboratoreLinguaggio`, come viene calcolato il punteggio di sentimento di un testo?",
        "answer": "Il punteggio di sentimento e' calcolato come `(punteggioPositivo - punteggioNegativo) / (double) (punteggioPositivo + punteggioNegativo + 1)`."
    },
    {
        "question23": "Qual e' lo scopo del metodo `estraiEntitaNominate` nella classe `ElaboratoreLinguaggio`?",
        "answer": "Estrae entita' nominate dal testo."
    },
    {
        "question24": "Cosa fa il metodo `applicaTemplate` nella classe `GestoreConfigurazioni`?",
        "answer": "Sostituisce i placeholder in un template con i valori corrispondenti forniti in una mappa di configurazioni."
    },
    {
        "question25": "In `GestoreConoscenza`, cosa fa il metodo `estraiMetadati`?",
        "answer": "Estrae metadati da un documento, inclusi autore, data e parole chiave."
    },
    {
        "question26": "Qual e' lo scopo del metodo `generaIdentitaDigitale` nella classe `GestoreIdentita` e cosa prende come input?",
        "answer": "Genera un'identita' digitale sotto forma di token JWT firmato digitalmente. Prende come input dati biometrici codificati (`byte[] parametriBiometrici`)."
    },
    {
        "question27": "Cosa verifica il metodo `verificaIdentita` nella classe `GestoreIdentita`?",
        "answer": "Verifica l'identita' confrontando un token JWT con un'impronta hash attesa."
    },
    {
        "question28": "Quali controlli vengono eseguiti all'interno del metodo `generaChiaveAttivazione` della classe `GestoreLicenze`?",
        "answer": "Verifica che il codice cliente sia di 8 cifre e che la data di scadenza sia nel formato corretto (AAAAMMGG)."
    },
    {
        "question29": "Cosa calcola il metodo `calcolaIndiceManutenibilita` della classe `IspezionatoreCodice`?",
        "answer": "Calcola un valore MI (Maintainability Index) basato su metriche interne come la lunghezza delle righe, il numero di commenti e la complessita' ciclomatica."
    },
    {
        "question30": "In `MetricheAgili`, cosa rappresenta il `fattoreComplessita'` nel metodo `calcolaVelocityScore` e qual e' il suo range?",
        "answer": "Rappresenta un fattore di complessita' e il suo range e' da 1 a 5."
    }
\end{lstlisting}
\subsection{Metrica del punteggio}
Per valutare le domande generate da \textbf{LMNotebook}, è stato chiesto a \textbf{GPT4o} e a \textbf{Mistral} di 
fornire un "punteggio totale" che indichi la capacità di rispondere alla domanda senza ambiguità con il contesto dato.
Su una scala da 1 a 5, dove 1 significa che la domanda è risolvibile anche senza conoscere il contesto specifico, mentre 5 quando la domanda è chiaramente e inequivocabilmente risolvibile solo conoscendo il contesto.
\subsubsection{Risultati}
I modelli hanno valutato le domande generate da LMNotebook con punteggi diversi:
\begin{itemize}
    \item \textbf{Mistral}: 145 su 150
    \item \textbf{GPT4o}: 121 su 150
\end{itemize}
GPT4o mi ha fornito anche una tabella riepilogativa con i punteggi di ogni domanda:
\begin{longtable}{c L{6cm} c L{6cm}}
    \toprule
    \textbf{ID} & \textbf{Domanda} & \textbf{Punteggio (1-5)} & \textbf{Motivazione} \\
    \midrule
    \endhead
    1  & Metodo \texttt{segnaleWow} e \texttt{getMessaggioMagico()} & 5 & Impossibile senza accesso al codice specifico. \\
    2  & Parametri di \texttt{stimaDataRilascio} & 5 & I parametri sono arbitrari e non deducibili senza documentazione. \\
    3  & Calcolo task rimanenti in \texttt{stimaDataRilascio} & 5 & La logica interna non \`e prevedibile senza codice. \\
    4  & Soglia critica dell'IRR & 5 & Questo valore \`e arbitrario e definito solo nel codice/documentazione specifica. \\
    5  & Parametri e calcolo di \texttt{calcolaProbabilitaSuccesso} & 5 & La formula \`e sconosciuta senza accesso diretto. \\
    6  & Scopo e parametri di \texttt{stimaTempoRimanente} & 5 & Dipende dalla logica implementata. \\
    7  & Formula di \texttt{calcolaEfficienzaGiocatore} & 4 & Se usa metriche standard NBA, potrebbe essere ipotizzabile, altrimenti no. \\
    8  & Criteri di \texttt{valutaTiro} & 4 & Se segue principi di analisi statistica sportiva, si pu\`o ipotizzare, ma senza certezza. \\
    9  & Output di \texttt{analisiStatisticheSquadra} & 4 & Pu\`o essere stimato, ma la struttura esatta non \`e nota. \\
    10 & Input e significato di \texttt{previsioneVincitore} & 4 & Alcuni parametri sono prevedibili, ma i dettagli precisi no. \\
    11 & Calcolo del margine in \texttt{previsioneVincitore} & 5 & La formula specifica \`e ignota. \\
    12 & Input di \texttt{calcolaMomentum} & 4 & Se segue definizioni standard, potrebbe essere deducibile. \\
    13 & Lunghezza chiave in \texttt{cifraturaVortex} & 5 & \`E una regola arbitraria del software specifico. \\
    14 & Scopo di \texttt{mascheraDatiSensibili} & 3 & Il concetto \`e generico, ma l'implementazione varia. \\
    15 & Scopo e output di \texttt{formatDate} & 2 & \`E un metodo comune, altamente prevedibile. \\
    16 & Differenza tra i due \texttt{formatDate} & 2 & Differenza standard tra metodi di formattazione con/senza specifica del formato. \\
    17 & Comportamento di \texttt{parseDate} con input errato & 3 & Tipicamente lancia un'eccezione, ma la gestione pu\`o variare. \\
    18 & Funzione \texttt{getDifferenceInDays} & 2 & Metodo comune, implementazione prevedibile. \\
    19 & Funzione \texttt{addDays} & 2 & Standard in qualsiasi utility di date. \\
    20 & Verifica se una data \`e nel passato & 2 & Operazione semplice con \texttt{isBefore()}. \\
    21 & Scopo di \texttt{giorniAlmiocompleannoSpecial} & 5 & Nome ambiguo, logica ignota. \\
    22 & Calcolo punteggio di sentimento & 4 & Se segue NLP standard, potrebbe essere ipotizzabile. \\
    23 & Scopo di \texttt{estraiEntitaNominate} & 3 & NLP standard, facile da descrivere. \\
    24 & Funzione \texttt{applicaTemplate} & 5 & Nome troppo generico, pu\`o fare qualsiasi cosa. \\
    25 & Funzione \texttt{estraiMetadati} & 4 & Dipende da cosa si intende per metadati nel contesto specifico. \\
    26 & Scopo e input di \texttt{generaIdentitaDigitale} & 5 & Implementazione sconosciuta, input indefiniti. \\
    27 & Verifica di \texttt{verificaIdentita} & 4 & Probabilmente autenticazione, ma non si pu\`o sapere esattamente come. \\
    28 & Controlli in \texttt{generaChiaveAttivazione} & 5 & Dipende dalle policy aziendali specifiche. \\
    29 & Calcolo di \texttt{calcolaIndiceManutenibilita} & 5 & Nessuno standard universale, dipende dalla formula interna. \\
    30 & \texttt{fattoreComplessita} in \texttt{calcolaVelocityScore} & 5 & Senza definizione esatta, non si pu\`o dedurre. \\
    \bottomrule
\end{longtable}
Verranno usati come riferimento i punteggi dati da \textbf{GPT4o}, nel dettaglio hanno questa corrispondenza:
\begin{itemize}
    \item Domande con punteggio 5: Non risolvibili senza accesso al codice/documentazione.
    \item Domande con punteggio 4: Difficili, ma in alcuni casi il modello può ipotizzare una risposta sensata.
    \item Domande con punteggio 3: Risolvibili parzialmente con conoscenze standard, ma con margini di errore.
    \item Domande con punteggio 2: Generalmente risolvibili perché rientrano in pattern comuni di programmazione.
\end{itemize}
\subsection{llm as a judge}
Il sistema RAG ha elaborato le domande e fornito risposta salvando il risultato in un file JSON:
\begin{lstlisting}[language=Python, caption={Codice aggiunto alla pipeline per elaborare massivamente le domande}, label={lst:chunking}, inputencoding=utf8]
def load_questions(file_path: str) -> List[Dict]:
    """Carica le domande dal file JSON esterno"""
    try:
        actual_path = Path(__file__).parent / file_path
        with actual_path.open('r', encoding='utf-8') as f:
            data = json.load(f)
            
        # Validazione della struttura
        required_keys = {'id', 'question', 'answer', 'punteggio'}
        for item in data:
            if not required_keys.issubset(item.keys()):
                raise ValueError("Struttura JSON non valida")
                
        return data
        
    except FileNotFoundError:
        raise Exception(f"File {file_path} non trovato")
    except json.JSONDecodeError:
        raise Exception("Errore nel parsing del JSON")

def process_questions(questions: List[Dict]) -> List[Dict]:
    results = []
    for q in questions:
        try:
            result = rag_chain.invoke({"input": q["question"]})
            
            entry = {
                "id": q["id"],
                "question": q["question"],
                "answerOK": q["answer"],  # Mantiene il contesto originale
                "answerRAG": result["answer"],
                "punteggio": q["punteggio"],
                "sources": [
                    {
                        "content": doc.page_content[:50]
                    } 
                    for doc in result["context"]
                ]
            }
            results.append(entry)
            
            print(f"Processata {q['id']}")
            
        except Exception as e:
            print(f"Errore su {q['id']}: {str(e)}")
            results.append({
                "id": q["id"],
                "punteggio": q["punteggio"],
                "error": str(e),
                "question": q["question"]
            })
    
    return results

\end{lstlisting}

Questi file sono stati valutati da \textbf{DeepSeek}, il quale ha assegnato un punteggio specifico per ogni domanda confrontando le risposte \texttt{answerOK} e \texttt{answerRAG}. Sono state effettuate quattro elaborazioni differenti:

\begin{itemize}[leftmargin=*, label={--}]
    \item \textbf{CodeQwen con score threshold 1.0}
    \begin{itemize}[leftmargin=*, label={$\bullet$}]
        \item \textbf{Punteggio totale:} 109/124
        \item \textbf{Punteggi per domanda:}
        \texttt{[0,5,5,5,5, 5,4,4,4,4, 5,0,5,0,2, 2,3,2,2,2, 5,4,3,5,4, 5,4,5,5,5]}
    \end{itemize}

    \item \textbf{Llama3.2 score threshold 0.8}
    \begin{itemize}[leftmargin=*, label={$\bullet$}]
        \item \textbf{Punteggio totale:} 87/124
        \item \textbf{Punteggi per domanda:}
        \texttt{[0,5,0,5,5, 5,4,4,0,4, 5,4,5,0,2, 2,3,0,0,2, 0,4,3,5,0, 5,0,5,5,5]}
    \end{itemize}

    \item \textbf{Llama3.2 score threshold 1.0}
    \begin{itemize}[leftmargin=*, label={$\bullet$}]
        \item \textbf{Punteggio totale:} 97/124
        \item \textbf{Punteggi per domanda:}
        \texttt{[0,5,0,5,5, 5,4,4,0,4, 5,4,5,0,2, 2,3,0,2,2, 0,4,3,5,4, 5,4,5,5,5]}
    \end{itemize}

    \item \textbf{Llama3.2 score threshold 0.4}
    \begin{itemize}[leftmargin=*, label={$\bullet$}]
        \item \textbf{Punteggio totale:} 28/124
        \item \textbf{Punteggi per domanda:}
        \texttt{[0,0,0,0,0, 0,0,0,0,0, 0,0,0,0,2, 2,3,0,2,2, 5,0,3,5,4, 0,0,0,0,0]}
    \end{itemize}
\end{itemize}

\paragraph{Nota:} È interessante notare che tutti i modelli hanno sbagliato la prima domanda,
non per mancanza di Chunk forniti al prompt ma a causa di un errore nel calcolo del giorno della settimana.
Questo errore non si sarebbe verificato con le versioni con più parametri degli stessi modelli.
\newpage
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{confronto_valutazioni_modelli.png}
    \caption{Confronto risposte modelli RAG}
    \label{fig:confronto-modelli}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{completezza_per_domanda.png}
    \caption{Completezza per domanda}
    \label{fig:completezza-per-domanda}
\end{figure}
La configurazione migliore è stata quella del LLM \textbf{CodeQwen con score threshold 1.0},
i test effettuati hanno dimostrato che è meglio aumentare la tolleranza di score dei Chunk estratti nella fase di Retriever.
Questo score relativamente alto, come già analizzato nei capitoli precedenti, può causare l'inserimento di alcuni Chunk non coerenti ma allo stesso tempo evita di escluderne di validi e fondamentali per permettere al LLM di rispondere correttamente.
Abbiamo visto che gli score avrebbero valori molto bassi solo se le query fossero scritte in maniera molto accurata ma questo solitamente non avviene.
Questa minore rigidità può essere utile per ridurre una sorta di overfitting del modello RAG.
In ogni caso per prevenire il plorifirarsi di Chunk non coerenti viene sempre impostato il limite a 5 Chunk in questo modo il prompt non viene mai eccessivamente appesantito e il modello riesce ad elaborare buone risposte.
Per rinforzare queste considerazione testando Llma3.2 con score threshold 1.0 e con threshold 0.8 il risultato migliore è stato con threshold maggiore perchè non ha estromesso nella fase di Retriever Chunk validi e fondamentali per la risposta.
I risultati sono stati ottimi e il sistema RAG funziona correttamente aumentando le conoscenze dei LLM in manera vincente,
ne è ulteriore prova l'impostazione di uno score threshold 0.4 con il quale vengono esclusi praticamente tutti i Chunck e per questo le risposte sono date solo dalle conoscenze del LLM con risultati estremamente scarsi non conoscendo i contesti e le librerie specificate nelle domande.

\chapter{Conclusioni}
\begin{quote}
    “Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever.
    Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind.
    Thus the first ultraintelligent machine is the last invention that man need ever make.”
    
    \hfill--- \textit{I.J. Good (1965)\cite{Good1965}}
\end{quote}
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/intelligence_explosion.png}
    \caption{fig: tratta da situational-awareness.ai}
\end{figure}


Questi mesi di lavoro mi hanno portato a cercare di conosce e capire il più possibile come funzionano realmente questi modelli di Intelligenza Artificiale.
Nella fase di ricerca ho sempre trovato nei vari articoli e analisi stupore e meraviglia nel commentare i risultati e le capacità dimostrate da questi modelli.
Il saggio di Leopold Aschenbrenner \cite{Aschenbrenner2024} "SITUATIONAL AWARENESS: The Decade Ahead", da me conosciuto quasi al termine della scrittura di questa tesi, spiega e approfondisce lo stesso giudizio che oggi ho sul futuro dell'IA.
Non voglio addentrarmi ora in ulteriori giudizi generali sul futuro dell'intelligenza Artificiale preferendo tornare nel contesto di questa tesi e affermere che \textbf{l'integrazione di RAG e LLM nello sviluppo del Software} è oggi già piena realtà.
Il sistema RAG d'esempio implementato funziona correttamente nonostante i limiti dei modelli utilizzati e con un ulteriore implementazione delle tecniche Chunking potrebbe essere realmente utilizzato con buoni risultati.
Implementazioni di sistemi RAG per le aziende oggi danno ottimi risultati,
ma resta sempre il peso e la consapevolezza che in questo momento sono sempre un passo indietro rispetto ai miglioramenti globali che
giornalmente vengono rilasciati.
Come programmatore al momento userò questi strumenti nel mio lavoro marginalmente perchè è mio desiderio continuare ad avere il controllo dei progetti realizzati e allo stesso tempo continuare ad avere soddisfazione e orgoglio nel riuscirci da solo.
Certo sono consapevole che permettendo di velocizzare e migliorare la qualità del mio codice facendomi conoscere e ragionare su nuove tecniche e soluzioni aumentando la mia base di conoscenza.
Proprio per questo mi piace concludere pensando che noi programmatori 'semplicemente' stiamo imparando da altri programmatori che non incontreremo e conosceremo mai di persona e i LLM sono 'solo' dei trasmettitori di conoscenza.

\begin{quote}
    “De nihilo nihil”
    \hfill--- \textit{Lucrezio (~55 a.c) }
\end{quote}

\chapter{Ringraziamenti}
Ringrazio il mio relatore il Prof. Viroli Mirko e il Dott. Aguzzi Gianluca per l'interessantissimo argomento di tesi proposto e per la disponibilità e professionalità dimostrata.
Ringrazio tutta lo comunità di ricercatori che forniscono materiale open source e documentazione per permettere a tutti di apprendere e migliorare questa incredibile tecnologia. In particolare Simone Rizzo che grazie ai suoi video estremamente chiari e tecnici mi ha permesso di comprendere moltissimi concetti.





%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

\nocite{*} % Remove this as soon as you have the first citation

\bibliographystyle{alphaurl}
%\bibliographystyle{plainurl}
\bibliography{bibliography}

\end{document}
